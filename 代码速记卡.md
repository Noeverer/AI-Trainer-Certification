# ğŸ¯ äººå·¥æ™ºèƒ½è®­ç»ƒå¸ˆä¸‰çº§ - ä»£ç é€Ÿè®°å¡

> ğŸ“± ä¸“ä¸ºæ‰‹æœºé˜…è¯»ä¼˜åŒ– | â° è€ƒå‰å¿…èƒŒ

---

## âš ï¸ é‡è¦æç¤º

**è€ƒè¯•æ—¶ `import` è¯­å¥å·²ç»å†™å¥½ï¼Œä¸éœ€è¦ä½ å†™ï¼**

ä½ åªéœ€è¦ä¸“æ³¨äº**å¡«å†™æ ¸å¿ƒä»£ç é€»è¾‘**å³å¯ã€‚

---

## ğŸ”¥ æ¨¡æ¿1: æ•°æ®åˆ†æç»Ÿè®¡ (1.1.X)

### å®Œæ•´ä»£ç æ¨¡æ¿

```python
import pandas as pd
import numpy as np

# 1ï¸âƒ£ è¯»å–æ•°æ®
data = pd.read_csv('xxx.csv')

# 2ï¸âƒ£ æ¡ä»¶åˆ†ç±»ï¼ˆç”¨np.whereï¼‰
data['é£é™©ç­‰çº§'] = np.where(
    data['ä½é™¢å¤©æ•°'] > 7, 
    'é«˜é£é™©', 
    'ä½é£é™©'
)

# 3ï¸âƒ£ ç»Ÿè®¡æ•°é‡å’Œå æ¯”
counts = data['é£é™©ç­‰çº§'].value_counts()
ratio = counts / len(data)
print(counts)
print(ratio)

# 4ï¸âƒ£ åˆ†ç®±æ“ä½œï¼ˆç”¨pd.cutï¼‰
bins = [0, 18.5, 24, 28, np.inf]
labels = ['åç˜¦', 'æ­£å¸¸', 'è¶…é‡', 'è‚¥èƒ–']
data['BMIåˆ†ç»„'] = pd.cut(
    data['BMI'], 
    bins=bins, 
    labels=labels, 
    right=False
)

# 5ï¸âƒ£ åˆ†ç»„ç»Ÿè®¡æ¯”ä¾‹
æ¯”ä¾‹ = data.groupby('BMIåˆ†ç»„')['é£é™©ç­‰çº§'].apply(
    lambda x: (x == 'é«˜é£é™©').mean()
)

# 6ï¸âƒ£ åˆ†ç»„ç»Ÿè®¡æ•°é‡
æ•°é‡ = data['BMIåˆ†ç»„'].value_counts()
```

### ğŸ”‘ å…³é”®ç‚¹é€Ÿè®°

| åŠŸèƒ½ | ä»£ç  | è®°å¿†å£è¯€ |
|------|------|----------|
| è¯»CSV | `pd.read_csv('æ–‡ä»¶.csv')` | pdè¯»csv |
| æ¡ä»¶åˆ¤æ–­ | `np.where(æ¡ä»¶, æ˜¯, å¦)` | npé—®å“ªé‡Œ |
| åˆ†ç®± | `pd.cut(åˆ—, bins, labels)` | pdåˆ‡åˆ† |
| åˆ†ç»„ç»Ÿè®¡ | `groupby('åˆ—').apply()` | æŒ‰ç»„åº”ç”¨ |
| è®¡æ•° | `value_counts()` | å€¼è®¡æ•° |

---

## ğŸ”¥ æ¨¡æ¿2: æ•°æ®æ¸…æ´—é¢„å¤„ç† (2.1.X)

### å®Œæ•´ä»£ç æ¨¡æ¿

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 1ï¸âƒ£ è¯»å–æ•°æ®
data = pd.read_csv('xxx.csv')
# æˆ–è¯»Excel
data = pd.read_excel('xxx.xlsx')

# 2ï¸âƒ£ æŸ¥çœ‹å‰5è¡Œ
print(data.head())

# 3ï¸âƒ£ æ£€æŸ¥ç¼ºå¤±å€¼
print(data.isnull().sum())

# 4ï¸âƒ£ åˆ é™¤ç¼ºå¤±å€¼
data = data.dropna()

# 5ï¸âƒ£ è½¬æ¢æ•°æ®ç±»å‹ï¼ˆå¤„ç†å¼‚å¸¸å€¼ï¼‰
data['åˆ—å'] = pd.to_numeric(
    data['åˆ—å'], 
    errors='coerce'
)
data = data.dropna()

# 6ï¸âƒ£ æ ‡å‡†åŒ–æ•°å€¼åˆ—
scaler = StandardScaler()
æ•°å€¼åˆ— = ['åˆ—1', 'åˆ—2', 'åˆ—3']
data[æ•°å€¼åˆ—] = scaler.fit_transform(data[æ•°å€¼åˆ—])

# 7ï¸âƒ£ é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
X = data[['ç‰¹å¾1', 'ç‰¹å¾2', 'ç‰¹å¾3']]
y = data['ç›®æ ‡åˆ—']

# 8ï¸âƒ£ åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42
)

# 9ï¸âƒ£ ä¿å­˜æ¸…æ´—åçš„æ•°æ®
data.to_csv('cleaned_data.csv', index=False)
```

### ğŸ”‘ å…³é”®ç‚¹é€Ÿè®°

| åŠŸèƒ½ | ä»£ç  | è®°å¿†å£è¯€ |
|------|------|----------|
| åˆ ç¼ºå¤±å€¼ | `data.dropna()` | dropæ‰NA |
| æ£€æŸ¥ç¼ºå¤± | `data.isnull().sum()` | æ˜¯ç©ºæ±‚å’Œ |
| ç±»å‹è½¬æ¢ | `pd.to_numeric(åˆ—, errors='coerce')` | è½¬æ•°å­—å¼ºåˆ¶ |
| æ ‡å‡†åŒ– | `scaler.fit_transform(åˆ—)` | æ‹Ÿåˆè½¬æ¢ |
| åˆ’åˆ†æ•°æ® | `train_test_split(X,y,test_size=0.2)` | è®­ç»ƒæµ‹è¯•åˆ† |

---

## ğŸ”¥ æ¨¡æ¿3: æœºå™¨å­¦ä¹ å»ºæ¨¡ (2.2.X)

### åˆ†ç±»ä»»åŠ¡æ¨¡æ¿ï¼ˆLogisticRegressionï¼‰

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import pickle

# 1ï¸âƒ£ åŠ è½½æ•°æ®
data = pd.read_csv('xxx.csv')

# 2ï¸âƒ£ é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
X = data.drop(['ç›®æ ‡åˆ—'], axis=1)
y = data['ç›®æ ‡åˆ—']

# 3ï¸âƒ£ åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42
)

# 4ï¸âƒ£ åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 5ï¸âƒ£ ä¿å­˜æ¨¡å‹
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# 6ï¸âƒ£ é¢„æµ‹
y_pred = model.predict(X_test)

# 7ï¸âƒ£ ä¿å­˜é¢„æµ‹ç»“æœ
pd.DataFrame(y_pred, columns=['é¢„æµ‹ç»“æœ']).to_csv(
    'results.txt', index=False
)

# 8ï¸âƒ£ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
report = classification_report(y_test, y_pred)
with open('report.txt', 'w') as f:
    f.write(report)

# 9ï¸âƒ£ è®¡ç®—å‡†ç¡®ç‡
accuracy = (y_test == y_pred).mean()
print(f"å‡†ç¡®ç‡: {accuracy:.2f}")
```

### å›å½’ä»»åŠ¡æ¨¡æ¿ï¼ˆRandomForestï¼‰

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pickle

# åˆ›å»ºéšæœºæ£®æ—å›å½’æ¨¡å‹
model = RandomForestRegressor(
    n_estimators=100,  # 100æ£µæ ‘
    random_state=42
)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°æŒ‡æ ‡
mse = mean_squared_error(y_test, y_pred)  # å‡æ–¹è¯¯å·®
r2 = r2_score(y_test, y_pred)  # RÂ²åˆ†æ•°
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

# ä¿å­˜æŠ¥å‘Š
with open('report.txt', 'w') as f:
    f.write(f'è®­ç»ƒé›†å¾—åˆ†: {train_score}\n')
    f.write(f'æµ‹è¯•é›†å¾—åˆ†: {test_score}\n')
    f.write(f'å‡æ–¹è¯¯å·®: {mse}\n')
    f.write(f'RÂ²åˆ†æ•°: {r2}\n')
```

### ğŸ”‘ å…³é”®ç‚¹é€Ÿè®°

| åŠŸèƒ½ | ä»£ç  | è®°å¿†å£è¯€ |
|------|------|----------|
| é€»è¾‘å›å½’ | `LogisticRegression(max_iter=1000)` | é€»è¾‘1000æ¬¡ |
| éšæœºæ£®æ— | `RandomForestRegressor(n_estimators=100)` | æ£®æ—100æ£µ |
| è®­ç»ƒ | `model.fit(X_train, y_train)` | æ¨¡å‹æ‹Ÿåˆ |
| é¢„æµ‹ | `model.predict(X_test)` | æ¨¡å‹é¢„æµ‹ |
| ä¿å­˜æ¨¡å‹ | `pickle.dump(model, open('x.pkl','wb'))` | æ³¡èœå­˜æ¨¡å‹ |
| åˆ†ç±»æŠ¥å‘Š | `classification_report(y_test, y_pred)` | åˆ†ç±»æŠ¥å‘Š |

---

## ğŸ”¥ æ¨¡æ¿4: ONNXæ¨¡å‹æ¨ç† (3.2.X)

### ç®€å•å›¾åƒåˆ†ç±»ï¼ˆå¦‚MNISTæ‰‹å†™æ•°å­—ï¼‰

```python
import onnxruntime as ort
import numpy as np
from PIL import Image

# 1ï¸âƒ£ åŠ è½½ONNXæ¨¡å‹
session = ort.InferenceSession('model.onnx')

# 2ï¸âƒ£ åŠ è½½å›¾ç‰‡
image = Image.open('img.png').convert('L')  # L=ç°åº¦å›¾

# 3ï¸âƒ£ é¢„å¤„ç†å›¾ç‰‡
image = image.resize((28, 28))  # è°ƒæ•´å¤§å°
image_array = np.array(image).astype(np.float32)

# 4ï¸âƒ£ æ·»åŠ ç»´åº¦ (batch, channel)
image_array = np.expand_dims(image_array, axis=0)  # batchç»´åº¦
image_array = np.expand_dims(image_array, axis=0)  # channelç»´åº¦

# 5ï¸âƒ£ è·å–è¾“å…¥åç§°å¹¶æ¨ç†
input_name = session.get_inputs()[0].name
output = session.run(None, {input_name: image_array})

# 6ï¸âƒ£ è·å–é¢„æµ‹ç»“æœ
predicted = np.argmax(output[0])
print(f"é¢„æµ‹ç»“æœ: {predicted}")
```

### å¤æ‚å›¾åƒåˆ†ç±»ï¼ˆå¦‚ResNetï¼‰

```python
import onnxruntime as ort
import numpy as np
import scipy.special
from PIL import Image

# é¢„å¤„ç†å‡½æ•°
def preprocess_image(image, size=224):
    image = image.resize((size, size))
    image = np.array(image).astype(np.float32)
    image = image / 255.0  # å½’ä¸€åŒ–
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    image = (image - mean) / std
    image = np.transpose(image, (2, 0, 1))  # HWC -> CHW
    image = np.expand_dims(image, axis=0)  # æ·»åŠ batchç»´åº¦
    return image.astype(np.float32)

# åŠ è½½æ¨¡å‹å’Œæ ‡ç­¾
session = ort.InferenceSession('resnet.onnx')
with open('labels.txt') as f:
    labels = [line.strip() for line in f.readlines()]

# åŠ è½½å¹¶å¤„ç†å›¾ç‰‡
image = Image.open('img.jpg').convert('RGB')
processed = preprocess_image(image)

# æ¨ç†
input_name = session.get_inputs()[0].name
output = session.run(None, {input_name: processed})[0]

# åº”ç”¨softmaxè·å–æ¦‚ç‡
probs = scipy.special.softmax(output, axis=-1)

# è·å–Top-5ç»“æœ
top5_idx = np.argsort(probs[0])[-5:][::-1]
for i, idx in enumerate(top5_idx):
    print(f"{i+1}: {labels[idx]} - {probs[0][idx]:.2%}")
```

### ğŸ”‘ å…³é”®ç‚¹é€Ÿè®°

| åŠŸèƒ½ | ä»£ç  | è®°å¿†å£è¯€ |
|------|------|----------|
| åŠ è½½æ¨¡å‹ | `ort.InferenceSession('x.onnx')` | ortä¼šè¯ |
| ç°åº¦å›¾ | `Image.open(x).convert('L')` | è½¬Lç°åº¦ |
| RGBå›¾ | `Image.open(x).convert('RGB')` | è½¬RGBå½©è‰² |
| è°ƒæ•´å¤§å° | `image.resize((28, 28))` | resizeå°ºå¯¸ |
| æ·»åŠ ç»´åº¦ | `np.expand_dims(arr, axis=0)` | æ‰©å±•ç»´åº¦ |
| è·å–è¾“å…¥å | `session.get_inputs()[0].name` | è·å–è¾“å…¥å |
| æ¨ç† | `session.run(None, {name: data})` | è¿è¡Œæ¨ç† |
| å–æœ€å¤§å€¼ç´¢å¼• | `np.argmax(output)` | å–æœ€å¤§ä¸‹æ ‡ |

---

## ğŸ“ 1.2.X æ–‡å­—é¢˜ç­”é¢˜æ¨¡æ¿

### ç¬¬ä¸€é—®ï¼šé—®é¢˜åˆ†æ

```
é—®é¢˜ä¸€ï¼šã€XXå‡†ç¡®æ€§/è¯†åˆ«å‡†ç¡®æ€§ä¸é«˜ã€‘

é—®é¢˜æè¿°ï¼š
ç³»ç»Ÿçš„[åŠŸèƒ½]å‡†ç¡®ç‡è¾ƒä½ï¼Œç»å¸¸å‡ºç°[å…·ä½“é”™è¯¯]ï¼Œ
å½±å“[ä¸šåŠ¡ç›®æ ‡]çš„å®ç°ã€‚

ç”¨æˆ·ä¸æ»¡åŸå› ï¼š
1. å†³ç­–è¯¯å¯¼ï¼šä¸å‡†ç¡®çš„ç»“æœå¯¼è‡´ç”¨æˆ·åšå‡ºé”™è¯¯å†³ç­–
2. èµ„æºæµªè´¹ï¼šåŸºäºé”™è¯¯ç»“æœçš„æŠ•å…¥æ— æ³•äº§ç”Ÿé¢„æœŸæ•ˆæœ
3. ä¿¡ä»»åº¦ä¸‹é™ï¼šç”¨æˆ·å¯¹ç³»ç»Ÿå¯é æ€§äº§ç”Ÿæ€€ç–‘
4. ä¸šåŠ¡æŸå¤±ï¼šæ— æ³•åŠæ—¶å‘ç°å’Œå¤„ç†é—®é¢˜

---

é—®é¢˜äºŒï¼šã€å“åº”é€Ÿåº¦æ…¢/å¤„ç†æ—¶é—´é•¿ã€‘

é—®é¢˜æè¿°ï¼š
ç³»ç»Ÿå¤„ç†[ä»»åŠ¡]çš„æ—¶é—´è¿‡é•¿ï¼Œç”¨æˆ·éœ€è¦ç­‰å¾…è¾ƒé•¿æ—¶é—´
æ‰èƒ½è·å¾—ç»“æœï¼Œå½±å“å®æ—¶å†³ç­–ã€‚

ç”¨æˆ·ä¸æ»¡åŸå› ï¼š
1. æ•ˆç‡ä½ä¸‹ï¼šæ— æ³•å¿«é€Ÿå“åº”å˜åŒ–ï¼Œé”™è¿‡æœ€ä½³å¤„ç†æ—¶æœº
2. ç”¨æˆ·ä½“éªŒå·®ï¼šé•¿æ—¶é—´ç­‰å¾…é™ä½ä½¿ç”¨æ„æ„¿
3. ç«äº‰åŠ£åŠ¿ï¼šç›¸æ¯”ç«äº‰å¯¹æ‰‹å“åº”èƒ½åŠ›å¤„äºä¸åˆ©åœ°ä½
4. ä¸šåŠ¡å»¶è¯¯ï¼šæ— æ³•åŠæ—¶å¤„ç†ç´§æ€¥æƒ…å†µ
```

### ç¬¬äºŒé—®ï¼šä¼˜åŒ–æ–¹æ¡ˆ

```
ä¼˜åŒ–æ–¹æ¡ˆæ¦‚è¿°ï¼š
é’ˆå¯¹[ç³»ç»Ÿåç§°]å­˜åœ¨çš„é—®é¢˜ï¼Œè®¾è®¡ç›¸åº”çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

å…³é”®å®æ–½æ­¥éª¤ï¼š

1ï¸âƒ£ æé«˜å‡†ç¡®æ€§
- å‡çº§ç®—æ³•å’ŒæŠ€æœ¯ï¼Œæå‡è¯†åˆ«/å¤„ç†å‡†ç¡®æ€§
- å»ºç«‹æ•°æ®æ ¡éªŒæœºåˆ¶ï¼Œç¡®ä¿æ•°æ®è´¨é‡

2ï¸âƒ£ åŠ å¿«å“åº”é€Ÿåº¦
- ä¼˜åŒ–ç³»ç»Ÿæ¶æ„ï¼Œç®€åŒ–å¤„ç†æµç¨‹
- å‡çº§ç¡¬ä»¶è®¾å¤‡ï¼Œæé«˜å¤„ç†èƒ½åŠ›

3ï¸âƒ£ å¢åŠ ä¸ªæ€§åŒ–åŠŸèƒ½
- å»ºç«‹ç”¨æˆ·ç”»åƒæ¨¡å‹ï¼Œäº†è§£ç”¨æˆ·éœ€æ±‚
- æä¾›ä¸ªæ€§åŒ–æœåŠ¡å’Œå®šåˆ¶åŒ–é€‰é¡¹

4ï¸âƒ£ æ”¹å–„ç³»ç»Ÿç¨³å®šæ€§
- å»ºç«‹ç³»ç»Ÿç›‘æ§æœºåˆ¶ï¼Œå®æ—¶ç›‘æ§çŠ¶æ€
- å»ºç«‹è‡ªåŠ¨æ•…éšœæ¢å¤æœºåˆ¶

é¢„æœŸä¼˜åŒ–æ•ˆæœï¼š
- å‡†ç¡®ç‡æ˜¾è‘—æå‡
- å“åº”é€Ÿåº¦å¤§å¹…æ”¹å–„
- ä¸ªæ€§åŒ–æœåŠ¡èƒ½åŠ›å¢å¼º
- ç³»ç»Ÿç¨³å®šæ€§æ˜æ˜¾æé«˜
```

---

## âš¡ è€ƒåœºæ€¥æ•‘é€Ÿè®°

### ä¸‡èƒ½å¼€å¤´
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
```

### è¯»æ•°æ®
```python
data = pd.read_csv('xxx.csv')      # CSVæ–‡ä»¶
data = pd.read_excel('xxx.xlsx')   # Excelæ–‡ä»¶
```

### æ•°æ®æ¸…æ´—ä¸‰è¿
```python
data = data.dropna()               # åˆ ç¼ºå¤±
data = data.drop_duplicates()      # åˆ é‡å¤
data.to_csv('clean.csv', index=False)  # ä¿å­˜
```

### æœºå™¨å­¦ä¹ ä¸‰è¿
```python
model.fit(X_train, y_train)        # è®­ç»ƒ
y_pred = model.predict(X_test)     # é¢„æµ‹
pickle.dump(model, open('m.pkl','wb'))  # ä¿å­˜
```

### ONNXæ¨ç†ä¸‰è¿
```python
session = ort.InferenceSession('x.onnx')  # åŠ è½½
output = session.run(None, {name: data})  # æ¨ç†
result = np.argmax(output[0])             # ç»“æœ
```

---

## ğŸ¯ è€ƒå‰æœ€åæ£€æŸ¥

### âœ… ç¡®è®¤ä½ èƒ½é»˜å†™

- [ ] `import pandas as pd`
- [ ] `import numpy as np`
- [ ] `from sklearn.model_selection import train_test_split`
- [ ] `pd.read_csv('xxx.csv')`
- [ ] `data.dropna()`
- [ ] `np.where(æ¡ä»¶, æ˜¯, å¦)`
- [ ] `pd.cut(åˆ—, bins, labels)`
- [ ] `train_test_split(X, y, test_size=0.2, random_state=42)`
- [ ] `model.fit(X_train, y_train)`
- [ ] `model.predict(X_test)`
- [ ] `pickle.dump(model, open('x.pkl', 'wb'))`
- [ ] `ort.InferenceSession('x.onnx')`
- [ ] `np.argmax(output)`

### âœ… 1.2.Xæ–‡å­—é¢˜å…³é”®è¯

**é—®é¢˜ç±»å‹**ï¼šå‡†ç¡®æ€§ã€å“åº”é€Ÿåº¦ã€ä¸ªæ€§åŒ–ã€ç¨³å®šæ€§

**ä¸æ»¡åŸå› **ï¼šå†³ç­–è¯¯å¯¼ã€èµ„æºæµªè´¹ã€ä¿¡ä»»åº¦ä¸‹é™ã€ä¸šåŠ¡æŸå¤±

**ä¼˜åŒ–æ–¹å‘**ï¼šå‡çº§ç®—æ³•ã€ä¼˜åŒ–æµç¨‹ã€å»ºç«‹æœºåˆ¶ã€æ”¹å–„ä½“éªŒ

---

ğŸ’ª **åŠ æ²¹ï¼ä½ ä¸€å®šèƒ½è¿‡çš„ï¼**
